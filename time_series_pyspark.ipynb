{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9b67558",
   "metadata": {},
   "source": [
    "# 1. Introduction and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270521a2",
   "metadata": {},
   "source": [
    "## Time Series, using PySpark\n",
    "PJM Interconnection LLC (PJM) is a regional transmission organization (RTO) in the United States. It is part of the Eastern Interconnection grid operating an electric transmission system serving all or parts of Delaware, Illinois, Indiana, Kentucky, Maryland, Michigan, New Jersey, North Carolina, Ohio, Pennsylvania, Tennessee, Virginia, West Virginia, and the District of Columbia.\n",
    "\n",
    "The hourly power consumption data comes from PJM's website and are in megawatts (MW)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc4b5f9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Library Imports and Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05964361",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "542b345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"TimeSeriesForecast\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d9cb9c",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e75f3826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Datetime: timestamp (nullable = true)\n",
      " |-- PJME_MW: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_path = \"./data/PJME_hourly.csv\"\n",
    "\n",
    "df = spark.read\\\n",
    "    .option(\"header\", True)\\\n",
    "    .option(\"inferSchema\", True)\\\n",
    "    .csv(data_path)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8bf28b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|           PJME_MW|\n",
      "+-------+------------------+\n",
      "|  count|            145366|\n",
      "|   mean|32080.222830648156|\n",
      "| stddev|6464.0121664127355|\n",
      "|    min|           14544.0|\n",
      "|    max|           62009.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0648c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Schema after conversions:\n",
      "root\n",
      " |-- Datetime: timestamp (nullable = true)\n",
      " |-- MW: double (nullable = true)\n",
      "\n",
      "+-------------------+-------+\n",
      "|Datetime           |MW     |\n",
      "+-------------------+-------+\n",
      "|2002-12-31 01:00:00|26498.0|\n",
      "|2002-12-31 02:00:00|25147.0|\n",
      "|2002-12-31 03:00:00|24574.0|\n",
      "|2002-12-31 04:00:00|24393.0|\n",
      "|2002-12-31 05:00:00|24860.0|\n",
      "+-------------------+-------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumnRenamed(\"PJME_MW\", \"MW\")\\\n",
    "       .withColumn(\"Datetime\", to_timestamp(col(\"Datetime\"), \"MM/dd/yyyy HH:mm\")) \\\n",
    "\n",
    "print(\"\\nSchema after conversions:\")\n",
    "df.printSchema()\n",
    "\n",
    "\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4aa6558a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original number of rows: 145366\n",
      "Number of rows after outlier removal: 145351\n",
      "+-------------------+-------+\n",
      "|Datetime           |MW     |\n",
      "+-------------------+-------+\n",
      "|2002-01-01 01:00:00|30393.0|\n",
      "|2002-01-01 02:00:00|29265.0|\n",
      "|2002-01-01 03:00:00|28357.0|\n",
      "|2002-01-01 04:00:00|27899.0|\n",
      "|2002-01-01 05:00:00|28057.0|\n",
      "+-------------------+-------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df_clean = df.filter(col(\"MW\") > 19000.0)\n",
    "\n",
    "\n",
    "df_sorted = df_clean.orderBy(\"Datetime\")\n",
    "\n",
    "print(f\"\\nOriginal number of rows: {df.count()}\")\n",
    "print(f\"Number of rows after outlier removal: {df_sorted.count()}\")\n",
    "\n",
    "\n",
    "df_sorted.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9307eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Schema with new time features:\n",
      "root\n",
      " |-- Datetime: timestamp (nullable = true)\n",
      " |-- MW: double (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- dayofweek: integer (nullable = true)\n",
      " |-- dayofyear: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- quarter: integer (nullable = true)\n",
      " |-- weekofyear: integer (nullable = true)\n",
      "\n",
      "+-------------------+-------+----+---------+-----+----+\n",
      "|Datetime           |MW     |hour|dayofweek|month|year|\n",
      "+-------------------+-------+----+---------+-----+----+\n",
      "|2002-01-01 01:00:00|30393.0|1   |3        |1    |2002|\n",
      "|2002-01-01 02:00:00|29265.0|2   |3        |1    |2002|\n",
      "|2002-01-01 03:00:00|28357.0|3   |3        |1    |2002|\n",
      "|2002-01-01 04:00:00|27899.0|4   |3        |1    |2002|\n",
      "|2002-01-01 05:00:00|28057.0|5   |3        |1    |2002|\n",
      "+-------------------+-------+----+---------+-----+----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, hour, dayofweek, month, year, dayofyear, weekofyear, quarter\n",
    "\n",
    "# Create a new DataFrame with all the required time features\n",
    "df_features = df_sorted.withColumn(\"hour\", hour(col(\"Datetime\"))) \\\n",
    "    .withColumn(\"dayofweek\", dayofweek(col(\"Datetime\"))) \\\n",
    "    .withColumn(\"dayofyear\", dayofyear(col(\"Datetime\"))) \\\n",
    "    .withColumn(\"month\", month(col(\"Datetime\"))) \\\n",
    "    .withColumn(\"year\", year(col(\"Datetime\"))) \\\n",
    "    .withColumn(\"quarter\", quarter(col(\"Datetime\"))) \\\n",
    "    .withColumn(\"weekofyear\", weekofyear(col(\"Datetime\")))\n",
    "\n",
    "# Display the new features created\n",
    "print(\"DataFrame Schema with new time features:\")\n",
    "df_features.printSchema()\n",
    "\n",
    "# Show the first few rows to confirm the features were added\n",
    "df_features.select(\"Datetime\", \"MW\", \"hour\", \"dayofweek\", \"month\", \"year\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf33bfc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Schema with new Lag features:\n",
      "root\n",
      " |-- Datetime: timestamp (nullable = true)\n",
      " |-- MW: double (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- dayofweek: integer (nullable = true)\n",
      " |-- dayofyear: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- quarter: integer (nullable = true)\n",
      " |-- weekofyear: integer (nullable = true)\n",
      " |-- lag1: double (nullable = true)\n",
      " |-- lag2: double (nullable = true)\n",
      " |-- lag3: double (nullable = true)\n",
      "\n",
      "\n",
      "First rows after adding and dropping NULLs (data starts after 3 years):\n",
      "+-------------------+-------+----+-------+-------+-------+\n",
      "|Datetime           |MW     |year|lag1   |lag2   |lag3   |\n",
      "+-------------------+-------+----+-------+-------+-------+\n",
      "|2004-12-28 07:00:00|37755.0|2004|24659.0|24574.0|30393.0|\n",
      "|2004-12-28 08:00:00|39493.0|2004|26076.0|24393.0|29265.0|\n",
      "|2004-12-28 09:00:00|40070.0|2004|28615.0|24860.0|28357.0|\n",
      "|2004-12-28 10:00:00|40030.0|2004|30876.0|26222.0|27899.0|\n",
      "|2004-12-28 11:00:00|39737.0|2004|31956.0|28702.0|28057.0|\n",
      "+-------------------+-------+----+-------+-------+-------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lag\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# 1. Define the Window Specification\n",
    "# This window applies over the entire dataset (no partition)\n",
    "# and orders the rows strictly by the 'Datetime' column.\n",
    "# This order is essential for the lag function to work correctly.\n",
    "window_spec = Window.orderBy(\"Datetime\")\n",
    "\n",
    "# Define the lag offsets in hours \n",
    "LAG_1_YR = 8736\n",
    "LAG_2_YR = 17472\n",
    "LAG_3_YR = 26208\n",
    "\n",
    "# 2. Apply the lag() function using the Window Specification\n",
    "df_lags = df_features.withColumn(\"lag1\", lag(col(\"MW\"), LAG_1_YR).over(window_spec)) \\\n",
    "                     .withColumn(\"lag2\", lag(col(\"MW\"), LAG_2_YR).over(window_spec)) \\\n",
    "                     .withColumn(\"lag3\", lag(col(\"MW\"), LAG_3_YR).over(window_spec))\n",
    "\n",
    "# 3. Handle Missing Values (NULLs)\n",
    "# The first 26208 rows (3 years) will have NULLs in the lag columns.\n",
    "# We drop these rows because a machine learning model can't train on NULLs.\n",
    "df_final_features = df_lags.dropna()\n",
    "\n",
    "print(\"DataFrame Schema with new Lag features:\")\n",
    "df_final_features.printSchema()\n",
    "\n",
    "print(\"\\nFirst rows after adding and dropping NULLs (data starts after 3 years):\")\n",
    "# Show the columns to confirm the lag values are present\n",
    "df_final_features.select(\"Datetime\", \"MW\", \"year\", \"lag1\", \"lag2\", \"lag3\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54849c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Schema after VectorAssembler:\n",
      "root\n",
      " |-- Datetime: timestamp (nullable = true)\n",
      " |-- MW: double (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- dayofweek: integer (nullable = true)\n",
      " |-- dayofyear: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- quarter: integer (nullable = true)\n",
      " |-- weekofyear: integer (nullable = true)\n",
      " |-- lag1: double (nullable = true)\n",
      " |-- lag2: double (nullable = true)\n",
      " |-- lag3: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+-------------------+-------+-------------------------------------------------------------+\n",
      "|Datetime           |MW     |features                                                     |\n",
      "+-------------------+-------+-------------------------------------------------------------+\n",
      "|2004-12-28 07:00:00|37755.0|[7.0,3.0,363.0,12.0,2004.0,4.0,53.0,24659.0,24574.0,30393.0] |\n",
      "|2004-12-28 08:00:00|39493.0|[8.0,3.0,363.0,12.0,2004.0,4.0,53.0,26076.0,24393.0,29265.0] |\n",
      "|2004-12-28 09:00:00|40070.0|[9.0,3.0,363.0,12.0,2004.0,4.0,53.0,28615.0,24860.0,28357.0] |\n",
      "|2004-12-28 10:00:00|40030.0|[10.0,3.0,363.0,12.0,2004.0,4.0,53.0,30876.0,26222.0,27899.0]|\n",
      "|2004-12-28 11:00:00|39737.0|[11.0,3.0,363.0,12.0,2004.0,4.0,53.0,31956.0,28702.0,28057.0]|\n",
      "+-------------------+-------+-------------------------------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# List all the feature columns we want to use for the model.\n",
    "FEATURE_COLS = [\n",
    "    \"hour\", \"dayofweek\", \"dayofyear\", \"month\", \"year\", \"quarter\", \"weekofyear\",\n",
    "    \"lag1\", \"lag2\", \"lag3\"\n",
    "]\n",
    "TARGET_COL = \"MW\" # Our energy consumption target\n",
    "\n",
    "# Initialize the VectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=FEATURE_COLS,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Apply the assembler transformation to create the features vector column\n",
    "df_model_ready = assembler.transform(df_final_features)\n",
    "\n",
    "# Show the resulting DataFrame structure\n",
    "print(\"DataFrame Schema after VectorAssembler:\")\n",
    "df_model_ready.printSchema()\n",
    "\n",
    "# Show the new features column\n",
    "df_model_ready.select(\"Datetime\", \"MW\", \"features\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644e62ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Train Set Size (before 2015-01-01): 87703 rows\n",
      "Test Set Size (on or after 2015-01-01): 31440 rows\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "# Define the cutoff date for the split\n",
    "CUTOFF_DATE = \"2015-01-01\"\n",
    "\n",
    "# Convert Datetime to Date for easy comparison \n",
    "df_split = df_model_ready.withColumn(\"Date\", to_date(col(\"Datetime\")))\n",
    "\n",
    "# Create the training set (data before the cutoff date)\n",
    "train_df = df_split.filter(col(\"Date\") < CUTOFF_DATE)\n",
    "\n",
    "# Create the testing set\n",
    "test_df = df_split.filter(col(\"Date\") >= CUTOFF_DATE)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Train Set Size (before {CUTOFF_DATE}): {train_df.count()} rows\")\n",
    "print(f\"Test Set Size (on or after {CUTOFF_DATE}): {test_df.count()} rows\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a0a81b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training with GBTRegressor...\n",
      "Model training complete.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# The target column ('MW') and the feature vector column ('features')\n",
    "# are automatically handled by the regressor.\n",
    "\n",
    "# Define the Gradient-Boosted Tree Regressor model\n",
    "# We set parameters similar to a robust XGBoost setup:\n",
    "gbt_regressor = GBTRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"MW\", # TARGET_COL from the previous step\n",
    "    maxIter=50,     # Number of trees/iterations (a typical default)\n",
    "    maxDepth=5      # Depth of each decision tree\n",
    ")\n",
    "\n",
    "print(\"Starting model training with GBTRegressor...\")\n",
    "\n",
    "# Fit the model to the training set (train_df)\n",
    "model = gbt_regressor.fit(train_df)\n",
    "\n",
    "print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a7daa5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 predictions:\n",
      "+-------------------+-------+-----------------+\n",
      "|Datetime           |MW     |prediction       |\n",
      "+-------------------+-------+-----------------+\n",
      "|2015-01-01 00:00:00|32802.0|34421.31684998449|\n",
      "|2015-01-01 01:00:00|31647.0|33213.17155640886|\n",
      "|2015-01-01 02:00:00|30755.0|33165.12554789061|\n",
      "|2015-01-01 03:00:00|30189.0|33056.68809190518|\n",
      "|2015-01-01 04:00:00|29890.0|33056.68809190518|\n",
      "+-------------------+-------+-----------------+\n",
      "only showing top 5 rows\n",
      "--------------------------------------------------\n",
      "✅ Final PySpark Model RMSE on Test Set: 4,001.13\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "predictions_df = model.transform(test_df)\n",
    "\n",
    "# Show the actual MW value vs. the predicted MW value\n",
    "print(\"\\nFirst 5 predictions:\")\n",
    "predictions_df.select(\"Datetime\", \"MW\", \"prediction\").show(5, truncate=False)\n",
    "\n",
    "# Initialize the evaluator to calculate RMSE\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"MW\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\" # Use Root Mean Squared Error\n",
    ")\n",
    "\n",
    "# Calculate the final RMSE score on the test set\n",
    "rmse_score = evaluator.evaluate(predictions_df)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"✅ Final PySpark Model RMSE on Test Set: {rmse_score:,.2f}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04b83ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import to_date, col\n",
    "\n",
    "# Add a simple Date column for filtering (easier than comparing Timestamps)\n",
    "df_split = df_model_ready.withColumn(\"Date\", to_date(col(\"Datetime\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e4f57fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 3-Fold Time Series Cross-Validation...\n",
      "\n",
      "--- FOLD 1 ---\n",
      "Train size: 87703 (up to 2015-01-01)\n",
      "Test size: 8760 (from 2015-01-01 to 2016-01-01)\n",
      "RMSE for Fold 1: 3,713.54\n",
      "\n",
      "--- FOLD 2 ---\n",
      "Train size: 96463 (up to 2016-01-01)\n",
      "Test size: 8784 (from 2016-01-01 to 2017-01-01)\n",
      "RMSE for Fold 2: 3,967.38\n",
      "\n",
      "--- FOLD 3 ---\n",
      "Train size: 105247 (up to 2017-01-01)\n",
      "Test size: 8760 (from 2017-01-01 to 2018-01-01)\n",
      "RMSE for Fold 3: 4,247.95\n",
      "\n",
      "--------------------------------------------------\n",
      "ALL FOLDS RMSEs: [3713.5388239589797, 3967.3814679357106, 4247.950879227678]\n",
      "** ROBUST AVERAGE RMSE: 3,976.29 **\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define the GBT Regressor (same as before)\n",
    "gbt_regressor = GBTRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"MW\",\n",
    "    maxIter=50,\n",
    "    maxDepth=5\n",
    ")\n",
    "\n",
    "# Define the training and test periods for each fold (e.g., three years for training, one year for testing)\n",
    "# We will test in 2015, 2016, and 2017.\n",
    "FOLD_CUTOFFS = [\n",
    "    # Fold 1: Train up to 2015-01-01, Test 2015\n",
    "    (\"2015-01-01\", \"2016-01-01\"), \n",
    "    # Fold 2: Train up to 2016-01-01, Test 2016\n",
    "    (\"2016-01-01\", \"2017-01-01\"), \n",
    "    # Fold 3: Train up to 2017-01-01, Test 2017\n",
    "    (\"2017-01-01\", \"2018-01-01\") \n",
    "]\n",
    "\n",
    "rmse_results = []\n",
    "evaluator = RegressionEvaluator(labelCol=\"MW\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "print(\"Starting 3-Fold Time Series Cross-Validation...\\n\")\n",
    "\n",
    "for i, (train_cutoff, test_cutoff) in enumerate(FOLD_CUTOFFS):\n",
    "    # 1. TEMPORAL SPLIT\n",
    "    # Train: Data before the first cutoff (e.g., before 2015-01-01)\n",
    "    train_fold_df = df_split.filter(col(\"Date\") < train_cutoff)\n",
    "    # Test: Data between the cutoffs (e.g., between 2015-01-01 and 2016-01-01)\n",
    "    test_fold_df = df_split.filter((col(\"Date\") >= train_cutoff) & (col(\"Date\") < test_cutoff))\n",
    "\n",
    "    # Check that both DataFrames have data\n",
    "    if train_fold_df.count() == 0 or test_fold_df.count() == 0:\n",
    "        print(f\"Skipping Fold {i+1}: Insufficient data for this period.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"--- FOLD {i+1} ---\")\n",
    "    print(f\"Train size: {train_fold_df.count()} (up to {train_cutoff})\")\n",
    "    print(f\"Test size: {test_fold_df.count()} (from {train_cutoff} to {test_cutoff})\")\n",
    "    \n",
    "    # 2. TRAINING\n",
    "    model_fold = gbt_regressor.fit(train_fold_df)\n",
    "    \n",
    "    # 3. PREDICTION & EVALUATION\n",
    "    predictions_fold_df = model_fold.transform(test_fold_df)\n",
    "    rmse_fold = evaluator.evaluate(predictions_fold_df)\n",
    "    \n",
    "    print(f\"RMSE for Fold {i+1}: {rmse_fold:,.2f}\\n\")\n",
    "    rmse_results.append(rmse_fold)\n",
    "\n",
    "# Calculate the final average RMSE\n",
    "if rmse_results:\n",
    "    avg_rmse = sum(rmse_results) / len(rmse_results)\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(f\"ALL FOLDS RMSEs: {rmse_results}\")\n",
    "    print(f\"** ROBUST AVERAGE RMSE: {avg_rmse:,.2f} **\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
