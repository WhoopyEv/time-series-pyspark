{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9b67558",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270521a2",
   "metadata": {},
   "source": [
    "## Distributed Time Series Forecasting with PySpark\n",
    "\n",
    "This project was developed with the primary goal of **practicing and mastering PySpark's distributed computing capabilities** as applied to **Time Series Forecasting**.\n",
    "\n",
    "### Methodology and Implementation\n",
    "\n",
    "Using an hourly energy consumption dataset (`PJME_hourly.csv`), this work was guided by the advanced forecasting techniques implemented in the Kaggle notebook, *PT2: Time Series Forecasting with XGBoost*.\n",
    "\n",
    "The key steps successfully implemented in PySpark include:\n",
    "\n",
    "1.  **Feature Engineering:** Extraction of standard time variables (hour, month, year) and, crucially, the creation of **Lag Features** (energy consumption values from 1, 2, and 3 years prior) using PySpark's **Window Functions** .\n",
    "2.  **Temporal Validation:** Instead of a simple random split, a **Manual Time Series Cross-Validation (TimeSeriesSplit)** strategy was implemented with 3 folds (testing on 2015, 2016, and 2017 data) to obtain a robust, time-respecting model evaluation.\n",
    "3.  **Distributed Modeling:** The **Gradient-Boosted Tree Regressor (GBTRegressor)**, a native algorithm from PySpark MLlib, was used to train the model across the distributed environment.\n",
    "\n",
    "### Key Result\n",
    "\n",
    "After applying the temporal cross-validation, the model achieved a:\n",
    "\n",
    "$$\\text{Robust Average RMSE: } 3,976.29 \\text{ MW}$$\n",
    "\n",
    "This result is comparable to those obtained in the local (Pandas/XGBoost) environment, confirming the successful migration of complex *Feature Engineering* techniques to a distributed processing framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc582428",
   "metadata": {},
   "source": [
    "# 2. Load and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc4b5f9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Library Imports and Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "05964361",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp, to_date, hour, dayofweek, month, year, dayofyear, weekofyear, quarter, lag\n",
    "\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator \n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "542b345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"TimeSeriesForecast\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d9cb9c",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e75f3826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Datetime: timestamp (nullable = true)\n",
      " |-- PJME_MW: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_path = \"./data/PJME_hourly.csv\"\n",
    "\n",
    "df = spark.read\\\n",
    "    .option(\"header\", True)\\\n",
    "    .option(\"inferSchema\", True)\\\n",
    "    .csv(data_path)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8bf28b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|           PJME_MW|\n",
      "+-------+------------------+\n",
      "|  count|            145366|\n",
      "|   mean|32080.222830648156|\n",
      "| stddev|6464.0121664127355|\n",
      "|    min|           14544.0|\n",
      "|    max|           62009.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cd0648c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Schema after conversions:\n",
      "root\n",
      " |-- Datetime: timestamp (nullable = true)\n",
      " |-- MW: double (nullable = true)\n",
      "\n",
      "+-------------------+-------+\n",
      "|Datetime           |MW     |\n",
      "+-------------------+-------+\n",
      "|2002-12-31 01:00:00|26498.0|\n",
      "|2002-12-31 02:00:00|25147.0|\n",
      "|2002-12-31 03:00:00|24574.0|\n",
      "|2002-12-31 04:00:00|24393.0|\n",
      "|2002-12-31 05:00:00|24860.0|\n",
      "+-------------------+-------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumnRenamed(\"PJME_MW\", \"MW\")\\\n",
    "       .withColumn(\"Datetime\", to_timestamp(col(\"Datetime\"), \"MM/dd/yyyy HH:mm\")) \\\n",
    "\n",
    "print(\"\\nSchema after conversions:\")\n",
    "df.printSchema()\n",
    "\n",
    "\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4aa6558a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original number of rows: 145366\n",
      "Number of rows after outlier removal: 145351\n",
      "+-------------------+-------+\n",
      "|Datetime           |MW     |\n",
      "+-------------------+-------+\n",
      "|2002-01-01 01:00:00|30393.0|\n",
      "|2002-01-01 02:00:00|29265.0|\n",
      "|2002-01-01 03:00:00|28357.0|\n",
      "|2002-01-01 04:00:00|27899.0|\n",
      "|2002-01-01 05:00:00|28057.0|\n",
      "+-------------------+-------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df_clean = df.filter(col(\"MW\") > 19000.0)\n",
    "\n",
    "\n",
    "df_sorted = df_clean.orderBy(\"Datetime\")\n",
    "\n",
    "print(f\"\\nOriginal number of rows: {df.count()}\")\n",
    "print(f\"Number of rows after outlier removal: {df_sorted.count()}\")\n",
    "\n",
    "\n",
    "df_sorted.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c370515c",
   "metadata": {},
   "source": [
    "# 3. Time-Based Feature Engineering\n",
    "The forecast horizon is the length of time into the future for which forecasts are to be prepared. These generally vary from short-term forecasting horizons (less than three months) to long-term horizons (more than two years)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9307eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Schema with new time features:\n",
      "root\n",
      " |-- Datetime: timestamp (nullable = true)\n",
      " |-- MW: double (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- dayofweek: integer (nullable = true)\n",
      " |-- dayofyear: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- quarter: integer (nullable = true)\n",
      " |-- weekofyear: integer (nullable = true)\n",
      "\n",
      "+-------------------+-------+----+---------+-----+----+\n",
      "|Datetime           |MW     |hour|dayofweek|month|year|\n",
      "+-------------------+-------+----+---------+-----+----+\n",
      "|2002-01-01 01:00:00|30393.0|1   |3        |1    |2002|\n",
      "|2002-01-01 02:00:00|29265.0|2   |3        |1    |2002|\n",
      "|2002-01-01 03:00:00|28357.0|3   |3        |1    |2002|\n",
      "|2002-01-01 04:00:00|27899.0|4   |3        |1    |2002|\n",
      "|2002-01-01 05:00:00|28057.0|5   |3        |1    |2002|\n",
      "+-------------------+-------+----+---------+-----+----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Create a new DataFrame with all the required time features\n",
    "df_features = df_sorted.withColumn(\"hour\", hour(col(\"Datetime\"))) \\\n",
    "    .withColumn(\"dayofweek\", dayofweek(col(\"Datetime\"))) \\\n",
    "    .withColumn(\"dayofyear\", dayofyear(col(\"Datetime\"))) \\\n",
    "    .withColumn(\"month\", month(col(\"Datetime\"))) \\\n",
    "    .withColumn(\"year\", year(col(\"Datetime\"))) \\\n",
    "    .withColumn(\"quarter\", quarter(col(\"Datetime\"))) \\\n",
    "    .withColumn(\"weekofyear\", weekofyear(col(\"Datetime\")))\n",
    "\n",
    "\n",
    "print(\"DataFrame Schema with new time features:\")\n",
    "df_features.printSchema()\n",
    "\n",
    "df_features.select(\"Datetime\", \"MW\", \"hour\", \"dayofweek\", \"month\", \"year\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8d0bf9",
   "metadata": {},
   "source": [
    "# 4. Engineering Lag Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cf33bfc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Schema with new Lag features:\n",
      "root\n",
      " |-- Datetime: timestamp (nullable = true)\n",
      " |-- MW: double (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- dayofweek: integer (nullable = true)\n",
      " |-- dayofyear: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- quarter: integer (nullable = true)\n",
      " |-- weekofyear: integer (nullable = true)\n",
      " |-- lag1: double (nullable = true)\n",
      " |-- lag2: double (nullable = true)\n",
      " |-- lag3: double (nullable = true)\n",
      "\n",
      "\n",
      "First rows after adding and dropping NULLs (data starts after 3 years):\n",
      "+-------------------+-------+----+-------+-------+-------+\n",
      "|Datetime           |MW     |year|lag1   |lag2   |lag3   |\n",
      "+-------------------+-------+----+-------+-------+-------+\n",
      "|2004-12-28 07:00:00|37755.0|2004|24659.0|24574.0|30393.0|\n",
      "|2004-12-28 08:00:00|39493.0|2004|26076.0|24393.0|29265.0|\n",
      "|2004-12-28 09:00:00|40070.0|2004|28615.0|24860.0|28357.0|\n",
      "|2004-12-28 10:00:00|40030.0|2004|30876.0|26222.0|27899.0|\n",
      "|2004-12-28 11:00:00|39737.0|2004|31956.0|28702.0|28057.0|\n",
      "+-------------------+-------+----+-------+-------+-------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the Window Specification\n",
    "# This window applies over the entire dataset (no partition)\n",
    "# and orders the rows strictly by the 'Datetime' column.\n",
    "# This order is essential for the lag function to work correctly.\n",
    "window_spec = Window.orderBy(\"Datetime\")\n",
    "\n",
    "# Define the lag offsets in hours \n",
    "LAG_1_YR = 8736\n",
    "LAG_2_YR = 17472\n",
    "LAG_3_YR = 26208\n",
    "\n",
    "# 2. Apply the lag() function using the Window Specification\n",
    "df_lags = df_features.withColumn(\"lag1\", lag(col(\"MW\"), LAG_1_YR).over(window_spec)) \\\n",
    "                     .withColumn(\"lag2\", lag(col(\"MW\"), LAG_2_YR).over(window_spec)) \\\n",
    "                     .withColumn(\"lag3\", lag(col(\"MW\"), LAG_3_YR).over(window_spec))\n",
    "\n",
    "# 3. Handle Missing Values (NULLs)\n",
    "# The first 26208 rows (3 years) will have NULLs in the lag columns.\n",
    "# We drop these rows because a machine learning model can't train on NULLs.\n",
    "df_final_features = df_lags.dropna()\n",
    "\n",
    "print(\"DataFrame Schema with new Lag features:\")\n",
    "df_final_features.printSchema()\n",
    "\n",
    "print(\"\\nFirst rows after adding and dropping NULLs (data starts after 3 years):\")\n",
    "# Show the columns to confirm the lag values are present\n",
    "df_final_features.select(\"Datetime\", \"MW\", \"year\", \"lag1\", \"lag2\", \"lag3\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b01b45",
   "metadata": {},
   "source": [
    "# 5. Preparation and Temporal Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54849c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Schema after VectorAssembler:\n",
      "root\n",
      " |-- Datetime: timestamp (nullable = true)\n",
      " |-- MW: double (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- dayofweek: integer (nullable = true)\n",
      " |-- dayofyear: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- quarter: integer (nullable = true)\n",
      " |-- weekofyear: integer (nullable = true)\n",
      " |-- lag1: double (nullable = true)\n",
      " |-- lag2: double (nullable = true)\n",
      " |-- lag3: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+-------------------+-------+-------------------------------------------------------------+\n",
      "|Datetime           |MW     |features                                                     |\n",
      "+-------------------+-------+-------------------------------------------------------------+\n",
      "|2004-12-28 07:00:00|37755.0|[7.0,3.0,363.0,12.0,2004.0,4.0,53.0,24659.0,24574.0,30393.0] |\n",
      "|2004-12-28 08:00:00|39493.0|[8.0,3.0,363.0,12.0,2004.0,4.0,53.0,26076.0,24393.0,29265.0] |\n",
      "|2004-12-28 09:00:00|40070.0|[9.0,3.0,363.0,12.0,2004.0,4.0,53.0,28615.0,24860.0,28357.0] |\n",
      "|2004-12-28 10:00:00|40030.0|[10.0,3.0,363.0,12.0,2004.0,4.0,53.0,30876.0,26222.0,27899.0]|\n",
      "|2004-12-28 11:00:00|39737.0|[11.0,3.0,363.0,12.0,2004.0,4.0,53.0,31956.0,28702.0,28057.0]|\n",
      "+-------------------+-------+-------------------------------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# List all the feature columns we want to use for the model.\n",
    "FEATURE_COLS = [\n",
    "    \"hour\", \"dayofweek\", \"dayofyear\", \"month\", \"year\", \"quarter\", \"weekofyear\",\n",
    "    \"lag1\", \"lag2\", \"lag3\"\n",
    "]\n",
    "TARGET_COL = \"MW\" # Our energy consumption target\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=FEATURE_COLS,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "\n",
    "df_model_ready = assembler.transform(df_final_features)\n",
    "\n",
    "\n",
    "print(\"DataFrame Schema after VectorAssembler:\")\n",
    "df_model_ready.printSchema()\n",
    "\n",
    "\n",
    "df_model_ready.select(\"Datetime\", \"MW\", \"features\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70061d41",
   "metadata": {},
   "source": [
    "# 6. Training and Evaluation with GBTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644e62ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Train Set Size (before 2015-01-01): 87703 rows\n",
      "Test Set Size (on or after 2015-01-01): 31440 rows\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "CUTOFF_DATE = \"2015-01-01\"\n",
    "\n",
    "# Convert Datetime to Date for easy comparison \n",
    "df_split = df_model_ready.withColumn(\"Date\", to_date(col(\"Datetime\")))\n",
    "\n",
    "# Create the training set (data before the cutoff date)\n",
    "train_df = df_split.filter(col(\"Date\") < CUTOFF_DATE)\n",
    "\n",
    "# Create the testing set\n",
    "test_df = df_split.filter(col(\"Date\") >= CUTOFF_DATE)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Train Set Size (before {CUTOFF_DATE}): {train_df.count()} rows\")\n",
    "print(f\"Test Set Size (on or after {CUTOFF_DATE}): {test_df.count()} rows\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0a81b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training with GBTRegressor...\n",
      "Model training complete.\n"
     ]
    }
   ],
   "source": [
    "# The target column ('MW') and the feature vector column ('features')\n",
    "# are automatically handled by the regressor.\n",
    "\n",
    "# Define the Gradient-Boosted Tree Regressor model\n",
    "# We set parameters similar to a robust XGBoost setup:\n",
    "gbt_regressor = GBTRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"MW\", # TARGET_COL from the previous step\n",
    "    maxIter=50,     # Number of trees/iterations (a typical default)\n",
    "    maxDepth=5      # Depth of each decision tree\n",
    ")\n",
    "\n",
    "print(\"Starting model training with GBTRegressor...\")\n",
    "model = gbt_regressor.fit(train_df)\n",
    "\n",
    "print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7daa5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 predictions:\n",
      "+-------------------+-------+-----------------+\n",
      "|Datetime           |MW     |prediction       |\n",
      "+-------------------+-------+-----------------+\n",
      "|2015-01-01 00:00:00|32802.0|34421.31684998449|\n",
      "|2015-01-01 01:00:00|31647.0|33213.17155640886|\n",
      "|2015-01-01 02:00:00|30755.0|33165.12554789061|\n",
      "|2015-01-01 03:00:00|30189.0|33056.68809190518|\n",
      "|2015-01-01 04:00:00|29890.0|33056.68809190518|\n",
      "+-------------------+-------+-----------------+\n",
      "only showing top 5 rows\n",
      "--------------------------------------------------\n",
      "âœ… Final PySpark Model RMSE on Test Set: 4,001.13\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "predictions_df = model.transform(test_df)\n",
    "\n",
    "# Show the actual MW value vs. the predicted MW value\n",
    "print(\"\\nFirst 5 predictions:\")\n",
    "predictions_df.select(\"Datetime\", \"MW\", \"prediction\").show(5, truncate=False)\n",
    "\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"MW\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\" # Use Root Mean Squared Error\n",
    ")\n",
    "\n",
    "# Calculate the final RMSE score on the test set\n",
    "rmse_score = evaluator.evaluate(predictions_df)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\" Final PySpark Model RMSE on Test Set: {rmse_score:,.2f}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7f54ee",
   "metadata": {},
   "source": [
    "# 7. Temporal Cross-Validation in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c04b83ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a simple Date column for filtering (easier than comparing Timestamps)\n",
    "df_split = df_model_ready.withColumn(\"Date\", to_date(col(\"Datetime\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4f57fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 3-Fold Time Series Cross-Validation...\n",
      "\n",
      "--- FOLD 1 ---\n",
      "Train size: 87703 (up to 2015-01-01)\n",
      "Test size: 8760 (from 2015-01-01 to 2016-01-01)\n",
      "RMSE for Fold 1: 3,713.54\n",
      "\n",
      "--- FOLD 2 ---\n",
      "Train size: 96463 (up to 2016-01-01)\n",
      "Test size: 8784 (from 2016-01-01 to 2017-01-01)\n",
      "RMSE for Fold 2: 3,967.38\n",
      "\n",
      "--- FOLD 3 ---\n",
      "Train size: 105247 (up to 2017-01-01)\n",
      "Test size: 8760 (from 2017-01-01 to 2018-01-01)\n",
      "RMSE for Fold 3: 4,247.95\n",
      "\n",
      "--------------------------------------------------\n",
      "ALL FOLDS RMSEs: [3713.5388239589797, 3967.3814679357106, 4247.950879227678]\n",
      "** ROBUST AVERAGE RMSE: 3,976.29 **\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define the GBT Regressor (same as before)\n",
    "gbt_regressor = GBTRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"MW\",\n",
    "    maxIter=50,\n",
    "    maxDepth=5\n",
    ")\n",
    "\n",
    "# Define the training and test periods for each fold (e.g., three years for training, one year for testing)\n",
    "# We will test in 2015, 2016, and 2017.\n",
    "FOLD_CUTOFFS = [\n",
    "    (\"2015-01-01\", \"2016-01-01\"), \n",
    "    (\"2016-01-01\", \"2017-01-01\"), \n",
    "    (\"2017-01-01\", \"2018-01-01\") \n",
    "]\n",
    "\n",
    "rmse_results = []\n",
    "evaluator = RegressionEvaluator(labelCol=\"MW\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "print(\"Starting 3-Fold Time Series Cross-Validation...\\n\")\n",
    "\n",
    "for i, (train_cutoff, test_cutoff) in enumerate(FOLD_CUTOFFS):\n",
    "    # 1. TEMPORAL SPLIT\n",
    "    # Train: Data before the first cutoff (e.g., before 2015-01-01)\n",
    "    train_fold_df = df_split.filter(col(\"Date\") < train_cutoff)\n",
    "    # Test: Data between the cutoffs (e.g., between 2015-01-01 and 2016-01-01)\n",
    "    test_fold_df = df_split.filter((col(\"Date\") >= train_cutoff) & (col(\"Date\") < test_cutoff))\n",
    "\n",
    "    # Check that both DataFrames have data\n",
    "    if train_fold_df.count() == 0 or test_fold_df.count() == 0:\n",
    "        print(f\"Skipping Fold {i+1}: Insufficient data for this period.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"--- FOLD {i+1} ---\")\n",
    "    print(f\"Train size: {train_fold_df.count()} (up to {train_cutoff})\")\n",
    "    print(f\"Test size: {test_fold_df.count()} (from {train_cutoff} to {test_cutoff})\")\n",
    "    \n",
    "    # 2. TRAINING\n",
    "    model_fold = gbt_regressor.fit(train_fold_df)\n",
    "    \n",
    "    # 3. PREDICTION & EVALUATION\n",
    "    predictions_fold_df = model_fold.transform(test_fold_df)\n",
    "    rmse_fold = evaluator.evaluate(predictions_fold_df)\n",
    "    \n",
    "    print(f\"RMSE for Fold {i+1}: {rmse_fold:,.2f}\\n\")\n",
    "    rmse_results.append(rmse_fold)\n",
    "\n",
    "# Calculate the final average RMSE\n",
    "if rmse_results:\n",
    "    avg_rmse = sum(rmse_results) / len(rmse_results)\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(f\"ALL FOLDS RMSEs: {rmse_results}\")\n",
    "    print(f\"** ROBUST AVERAGE RMSE: {avg_rmse:,.2f} **\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
